# House Price Prediction Script — Step‑by‑Step Explanation


1. Loads the housing dataset (e.g., Kaggle’s House Prices data) from `train.csv`.
2. Cleans missing values for numeric and categorical features.
3. Converts text categories into numeric columns (one‑hot encoding).
4. Splits the data into training and testing sets.
5. Trains a **Linear Regression** model to predict `SalePrice`.
6. Evaluates the model with **MSE, RMSE, and R²** and shows a scatter plot of Actual vs. Predicted prices.

---

## Step 1 — Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
```

**Why:**

* **pandas (pd)**: tabular data handling (CSV reading, DataFrame operations).
* **numpy (np)**: numerical operations and arrays; used here to calculate RMSE via `np.sqrt`.
* **matplotlib.pyplot (plt)** & **seaborn (sns)**: visualization; used to plot Actual vs Predicted prices.
* **scikit‑learn (sklearn)**: provides tools for train/test split, modeling (**LinearRegression**), and metrics.



## Step 2 — Load the dataset and basic inspection

data = pd.read_csv("train.csv")

# Show first 5 rows
print(data.head())

# Check shape of dataset
print("Dataset Shape:", data.shape)

# Check missing values
print(data.isnull().sum().head(20))
```

**What happens:**

* `pd.read_csv("train.csv")` reads the training data into a DataFrame.
* `head()` gives a quick peek at the first five rows—useful to verify columns like `SalePrice` exist.
* `shape` reports (rows, columns), so you know dataset size.
* `isnull().sum()` counts missing values per column (the `.head(20)` just prints the first 20 columns’ counts to keep the console output short).

**Why it matters:**

* You confirm the file path is correct, columns were parsed, and get an initial sense of data completeness.

---

## Step 3 — Separate numeric and categorical columns

numeric_cols = data.select_dtypes(include=['number']).columns
categorical_cols = data.select_dtypes(include=['object']).columns
```

**What happens:**

* Automatically identifies numeric vs. text (object) features.

**Why it matters:**

* Different data types require different cleaning strategies (e.g., mean for numeric, mode for categorical).

---

## Step 4 — Handle missing values

# Fill missing numeric values with mean
data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].mean())

# Fill missing categorical values with mode (most frequent value)
for col in categorical_cols:
    if data[col].isnull().sum() > 0:
        data[col] = data[col].fillna(data[col].mode()[0])

# Drop columns with too many missing values (optional for beginners)
data = data.dropna(axis=1, thresh=0.8*len(data))
```

**What happens:**

* **Numeric:** Missing values are imputed using the **column mean**.
* **Categorical:** Missing values are imputed using the **most frequent category** (mode) per column.
* **Optional drop:** Any column that has **fewer than 80% non‑missing values** is dropped (i.e., only keep columns with ≥80% non‑null entries).

**Why it matters:**

* Most ML models can’t handle NaNs. Imputation is a simple, fast baseline.

**Caveats:**

* Mean/mode imputation is simple but can bias distributions. For stronger baselines, consider **median** for skewed numeric features or advanced imputers (e.g., `sklearn.impute.SimpleImputer`, `IterativeImputer`).
* If a categorical column is **all NaN**, `mode()[0]` would fail; your `if` skips only when there are missing values, but not the all‑NaN edge case. In practice, Kaggle’s dataset won’t have all‑NaN categoricals, but it’s good to be aware.

---

## Step 5 — One‑Hot Encode categorical columns

# Convert categorical columns to numbers (One-Hot Encoding)
data = pd.get_dummies(data, drop_first=True)
```

**What happens:**

* Converts each categorical column into a set of binary indicator (0/1) columns.
* `drop_first=True` drops one level per category to avoid perfect multicollinearity (the “dummy variable trap”) in linear models.

**Why it matters:**

* ML models need numeric inputs; one‑hot encoding represents categories numerically without imposing an arbitrary order.

---

## Step 6 — Define features (X) and target (y)

# Target = SalePrice
y = data["SalePrice"]

# Features = all other columns except SalePrice
X = data.drop("SalePrice", axis=1)

print("Features Shape:", X.shape)
print("Target Shape:", y.shape)
```

**What happens:**

* Splits the dataset into **X** (inputs) and **y** (output/label). Here the target is `SalePrice`.

**Why it matters:**

* Keeps a clean separation between what you predict (y) and what you use to predict it (X).

---

## Step 7 — Train/Test split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

**What happens:**

* Randomly splits the data into **80% training** and **20% testing**.
* `random_state=42` ensures reproducible splits.

**Why it matters:**

* The test set simulates unseen data, providing an unbiased estimate of generalization performance.

---

## Step 8 — Initialize and train the model

# Initialize model
model = LinearRegression()

# Train model
model.fit(X_train, y_train)
```

**What happens:**

* Creates a **Linear Regression** estimator.
* Learns coefficients (weights) and an intercept that minimize **Mean Squared Error** on the training set.

**Why it matters:**

* Linear regression is a fast, interpretable baseline. Coefficients indicate how each feature linearly influences `SalePrice` (holding others constant).

**Note:**

* Linear Regression assumes linear relationships, no perfect multicollinearity, homoscedastic residuals, and roughly normal errors. Real housing data often violates some assumptions, so treat this as a baseline.

---

## Step 9 — Predict on the test set

# Predict house prices
y_pred = model.predict(X_test)

print("Predicted Prices:", y_pred[:5])
print("Actual Prices:", list(y_test[:5]))
```

**What happens:**

* Uses the trained model to predict `SalePrice` for unseen test examples.
* Prints the first few predictions alongside the actual values for a quick sanity check.

**How to read it:**

* Predictions should be in the ballpark of actual prices; large systematic gaps suggest under/overfitting or missing features.

---

## Step 10 — Evaluate the model

# Mean Squared Error (MSE) - lower is better
mse = mean_squared_error(y_test, y_pred)

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# R² Score - closer to 1 is better
r2 = r2_score(y_test, y_pred)

print("MSE:", mse)
print("RMSE:", rmse)
print("R2 Score:", r2)
```

**Metrics explained:**

* **MSE**: Average of squared errors. Penalizes large errors more. Lower is better.
* **RMSE**: Square root of MSE. Same unit as the target (`SalePrice`), easier to interpret.
* **R² (coefficient of determination)**: Proportion of variance in `SalePrice` explained by the model. 1.0 is perfect, 0.0 means “no better than predicting the mean”; negative means worse than mean.

**Interpreting numbers:**

* On the Kaggle House Prices data (in USD), a strong baseline RMSE is typically in the **\$30k–\$40k** range after log‑transform and regularization; this plain linear model without feature engineering will likely be worse—which is okay for a first pass.

---

## Step 11 — Visualize Actual vs Predicted

plt.figure(figsize=(8,6))
sns.scatterplot(x=y_test, y=y_pred)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual vs Predicted House Prices")
plt.show()
```

**What you expect:**

* Points should cluster around a **diagonal line** (where `Predicted = Actual`). The tighter the cluster, the better.

**Nice enhancement (optional):** add a 45° reference line to judge fit at a glance:

import numpy as np
lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]
plt.plot(lims, lims, linestyle='--')  # identity line
```

---

## Common errors & quick fixes

* **File not found:** Ensure `train.csv` is in your working directory or provide the full path.
* **ModuleNotFoundError:** Install packages:

  * `pip install pandas numpy matplotlib seaborn scikit-learn`
* **Memory errors after get\_dummies:** High‑cardinality categoricals can explode the number of columns. Consider frequency encoding or limit rare categories.

---

## Ideas to improve this baseline

1. **Log‑transform the target**: `y = np.log1p(SalePrice)` often stabilizes variance and improves linear model performance. Remember to exponentiate predictions with `np.expm1` when interpreting.
2. **Regularization**: Try **Ridge** and **Lasso** (`sklearn.linear_model`) to handle multicollinearity and reduce overfitting.
3. **Cross‑validation**: Use `cross_val_score` or `KFold` for more reliable performance estimates.
4. **Feature engineering**: Create features like `TotalSF = TotalBsmtSF + 1stFlrSF + 2ndFlrSF`, age of house at sale, quality interactions, etc.
5. **Pipelines**: Use `ColumnTransformer` + `Pipeline` to cleanly apply imputation/encoding and avoid data leakage.
6. **Diagnostics**: Plot residuals vs. fitted values; check for nonlinearity and heteroscedasticity.
7. **Model zoo**: Compare with **RandomForestRegressor**, **GradientBoostingRegressor**, or **XGBoost/LightGBM/CatBoost**.

---

## Takeaway

You’ve built a complete end‑to‑end regression baseline:

                                         load → clean → encode → split → train → evaluate → visualize. 
                                         
This structure is a strong foundation you can iterate on with better preprocessing, engineered features, and more powerful models.
